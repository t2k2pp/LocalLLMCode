# LocalLLM Code Configuration

[llm]
provider = "lmstudio"
model = "default"
context_size = 8192
stream = true
temperature = 0.7

[lmstudio]
server_url = "http://localhost:1234"
model = "default"

[azure]
api_key = ""
endpoint = ""
deployment_name = ""
api_version = "2024-02-15-preview"

[gemini]
api_key = ""
model = "gemini-pro"

[safety]
require_confirmation = true
allow_dangerous_commands = false
backup_before_edit = true

[experimental]
auto_refactoring = false
context_compression = true
memory_optimization = true
